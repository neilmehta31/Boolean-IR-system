<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>ir_assignment1 API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>ir_assignment1</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># -*- coding: utf-8 -*-
# &#34;&#34;&#34;IR_Assignment1.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1ByabD3y9BBb3Pwz8rA5wqrdYx2FthNsZ
# &#34;&#34;&#34;

# from google.colab import drive
# drive.mount(&#39;/content/drive&#39;)

#importing the necessary libraries
# import nltk
# nltk.download(&#39;stopwords&#39;)
# nltk.download(&#39;punkt&#39;)
# from nltk.corpus import stopwords
# from nltk.stem import WordNetLemmatizer, PorterStemmer
# from nltk.tokenize import sent_tokenize , word_tokenize
# import glob
# import re
# import os
# import numpy as np
# import re
# import sys
# Stopwords = set(stopwords.words(&#39;english&#39;))

def finding_all_unique_words_and_freq(words):
    &#34;&#34;&#34;
    finds all the unique words and their frequency from the given dataset of IR Corpus
    
    Args:
    
    `words`:  words processed from the document

    Returns:
    
    The frequency of the given word in the function
    &#34;&#34;&#34;
    words_unique = []
    word_freq = {}
    for word in words:
        if word not in words_unique:
            words_unique.append(word)
    for word in words_unique:
        word_freq[word] = words.count(word)
    return word_freq

def finding_freq_of_word_in_doc(word,words):
    &#34;&#34;&#34;
    Counts the frequency fof the word in the document

    Args:
    `word`: word form the document

    `words`:counts the frequency for the given in the document
    &#34;&#34;&#34;
    freq = words.count(word)
        
def remove_special_characters(text):
    &#34;&#34;&#34;
    This function removes the special charactes if any in the document so that it is easy to process the document.

    Return:

    The text after removing the special characters. 
    &#34;&#34;&#34;
    regex = re.compile(&#39;[^a-zA-Z0-9\s]&#39;)
    text_returned = re.sub(regex,&#39;&#39;,text)
    return text_returned

def fileOpeningProcessing():
    &#34;&#34;&#34;
    Opens  the file and sends for preprocessing
    such as removing special characters.

    &#34;&#34;&#34;
    all_words = []
    dict_global = {}
    # file_folder = &#39;/home/fb/Desktop/IR/assignment/IR_CORPUS&#39;
    idx = 1
    files_with_index = {}
    # list_files = [f for f in glob.glob(&#39;/content/IR_CORPUS/*.txt&#39;)]
    list_files = [f for f in glob.glob(&#39;/content/drive/MyDrive/IR_ASSIGNMENT/IR_CORPUS/*.txt&#39;)]

    # for file in glob.glob(file_folder):
    for file in list_files:
        # print(file)
        # print(idx)
        fname = file
        file = open(file , &#34;r&#34;)
        text = file.read()
        text = remove_special_characters(text)
        text = re.sub(re.compile(&#39;\d&#39;),&#39;&#39;,text)
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        words = [word for word in words if len(words)&gt;1]
        words = [word.lower() for word in words]
        words = [word for word in words if word not in Stopwords]
        # words = [PorterStemmer().stem(word) for word in words]
        dict_global.update(finding_all_unique_words_and_freq(words))
        files_with_index[idx] = os.path.basename(fname)
        idx = idx + 1
        
    unique_words_all = set(dict_global.keys())

class Node:
    &#34;&#34;&#34;
    Class for Node
    &#34;&#34;&#34;
    def __init__(self ,docId, freq = None):
        self.freq = freq
        self.doc = docId
        self.nextval = None
    
class SlinkedList:
    def __init__(self ,head = None):
        self.head = head

def preprocessing():
    &#34;&#34;&#34;
    The following function preprocesses the document IR Corpus.
    &#34;&#34;&#34;
    linked_list_data = {}
    for word in unique_words_all:
        linked_list_data[word] = SlinkedList()
        linked_list_data[word].head = Node(1,Node)
    word_freq_in_doc = {}
    idx = 1
    # list_files = [f for f in glob.glob(&#39;/content/IR_CORPUS/*txt&#39;)]
    list_files = [f for f in glob.glob(&#39;/content/drive/MyDrive/IR_ASSIGNMENT/IR_CORPUS/*.txt&#39;)]

    # for file in glob.glob(file_folder):
    for file in list_files:
        file = open(file, &#34;r&#34;)
        text = file.read()
        # print(text)
        # print(idx)
        text = remove_special_characters(text)
        text = re.sub(re.compile(&#39;\d&#39;),&#39;&#39;,text)
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        words = [word for word in words if len(words)&gt;1]
        words = [word.lower() for word in words]
        words = [word for word in words if word not in Stopwords]
        # words = [PorterStemmer().stem(word) for word in words]
        word_freq_in_doc = finding_all_unique_words_and_freq(words)
        for word in word_freq_in_doc.keys():
            linked_list = linked_list_data[word].head
            while linked_list.nextval is not None:
                linked_list = linked_list.nextval
            linked_list.nextval = Node(idx ,word_freq_in_doc[word])
        idx = idx + 1
        # print(text)

# list_files = [f for f in glob.glob(&#39;/content/IR_CORPUS/*.txt&#39;)]
# print(list_files)

# &#34;&#34;&#34;# Ignore&#34;&#34;&#34;

# from nltk.metrics.distance  import edit_distance
# from nltk.metrics.distance import jaccard_distance
# from nltk.util import ngrams
# query = input(&#39;Enter your query:&#39;)
# query = word_tokenize(query)
# connecting_words = []
# cnt = 1
# k = 0 #retrieve top k+1 words for edit distance
# different_words = []
# zeroes_and_ones = []
# edited_query_words = []
# zeroes_and_ones_of_all_words = []
# total_files = len(files_with_index)

# for word in query:
#     if word.lower() != &#34;and&#34; and word.lower() != &#34;or&#34; and word.lower() != &#34;not&#34;:
#         different_words.append(word.lower())
#     else:
#         connecting_words.append(word.lower())
# skip_edit_distance = [0]*len(different_words)

# #if in unique words
# i = 0
# for query_word in different_words: 
#     # print(query_word)
#     if query_word in unique_words_all:
#         skip_edit_distance[i] = 1
#         zeroes_and_ones = [0] * total_files
#         linkedlist = linked_list_data[query_word].head
#         # print(&#34;word:&#34;,word)
#         while linkedlist.nextval is not None:
#             zeroes_and_ones[linkedlist.nextval.doc - 1] = 1
#             linkedlist = linkedlist.nextval
#         print(&#34;unique available :&gt; zeroes_and_ones for &#34;,query_word,&#34; :&gt;&#34;,zeroes_and_ones)
#         zeroes_and_ones_of_all_words.append(zeroes_and_ones)
#     i+=1

# # if not in unique words
# for i,entry in enumerate(different_words):
#     # print(i,entry)
#     if not skip_edit_distance[i]:
#             temp = [(edit_distance(entry, w,substitution_cost = 2,transpositions = True),w) for w in unique_words_all]
#             all_words_sorted = sorted(temp)
#             # print(&#34;all_words_sorted :&#34;, all_words_sorted)
#             edited_query_words.append(all_words_sorted[0])


# print(&#34;edited_query_words after edit distance: &#34;,edited_query_words)
# for entry in edited_query_words:
#             if entry[1] in unique_words_all:
#                 zeroes_and_ones = [0] * total_files
#                 linkedlist = linked_list_data[entry[1]].head
#                 # print(word)
#                 while linkedlist.nextval is not None:
#                     zeroes_and_ones[linkedlist.nextval.doc - 1] = 1
#                     linkedlist = linkedlist.nextval
#                 print(&#34;edit dist :&gt; zeroes_and_ones for &#34;,entry[1],&#34; :&gt;&#34;,zeroes_and_ones)
#                 zeroes_and_ones_of_all_words.append(zeroes_and_ones)
#             # else : 
#             #     print(word,&#34; not found&#34;)
#             #     sys.exit()



# # print(&#34;zeroes_and_ones_of_all_words: &#34;,zeroes_and_ones_of_all_words)
# for word in connecting_words:
#     word_list1 = zeroes_and_ones_of_all_words[0]
#     word_list2 = zeroes_and_ones_of_all_words[1]
#     if word == &#34;and&#34;:
#         bitwise_op = [w1 &amp; w2 for (w1,w2) in zip(word_list1,word_list2)]
#         zeroes_and_ones_of_all_words.remove(word_list1)
#         zeroes_and_ones_of_all_words.remove(word_list2)
#         zeroes_and_ones_of_all_words.insert(0, bitwise_op);
#     elif word == &#34;or&#34;:
#         bitwise_op = [w1 | w2 for (w1,w2) in zip(word_list1,word_list2)]
#         zeroes_and_ones_of_all_words.remove(word_list1)
#         zeroes_and_ones_of_all_words.remove(word_list2)
#         zeroes_and_ones_of_all_words.insert(0, bitwise_op);
#     elif word == &#34;not&#34;:
#         bitwise_op = [not w1 for w1 in word_list2]
#         bitwise_op = [int(b == True) for b in bitwise_op]
#         zeroes_and_ones_of_all_words.remove(word_list2)
#         zeroes_and_ones_of_all_words.remove(word_list1)
#         bitwise_op = [w1 &amp; w2 for (w1,w2) in zip(word_list1,bitwise_op)]
#         zeroes_and_ones_of_all_words.insert(0, bitwise_op);


# files = []    
# lis = zeroes_and_ones_of_all_words[0]
# print(&#34;final zeros after binary operation:&#34;,lis)
# cnt = 1
# for index in lis:
#     if index == 1:
#         files.append(files_with_index[cnt])
#     cnt = cnt+1
    
# print(files)

# &#34;&#34;&#34;#To be used&#34;&#34;&#34;

# list_test = []
# for word in unique_words_all:
#     if re.search(&#34;^bru.*s$&#34;,word):
#         list_test.append(word)
# print(list_test)

def wildcard(word):
    &#34;&#34;&#34;
    Utility function for wildcard searching
    &#34;&#34;&#34;
    wildcard_words = []
    word_list = word.split(&#34;*&#34;)
    re_query = &#34;^&#34;+ word_list[0]+&#34;.*&#34;+word_list[1]+&#34;$&#34;
    # print(re_query)
    for word in unique_words_all:
        if re.search(re_query,word):
            wildcard_words.append(word)        
    return wildcard_words

def wildcard_search(word):
    &#34;&#34;&#34;
    This function searches the query using wildcard searching if the token/word contains * in it. 
    &#34;&#34;&#34;
    total_files = len(files_with_index)
    zeroes_and_ones_of_all_words = []
    # word = &#34;br*us&#34;
    #if in unique words
    i = 0
    for query_word in wildcard(word): 
        # print(query_word)
        if query_word in unique_words_all:
            # skip_edit_distance[i] = 1
            zeroes_and_ones = [0] * total_files
            linkedlist = linked_list_data[query_word].head
            # print(&#34;word:&#34;,word)
            while linkedlist.nextval is not None:
                zeroes_and_ones[linkedlist.nextval.doc - 1] = 1
                linkedlist = linkedlist.nextval
            print(&#34;unique available :&gt; zeroes_and_ones for &#34;,query_word,&#34; :&gt;&#34;,zeroes_and_ones)
            zeroes_and_ones_of_all_words.append(zeroes_and_ones)
        i+=1

    res = [0] * total_files
    for zr_and_ons in zeroes_and_ones_of_all_words:
        res = boolean_or(res,zr_and_ons)
    return res

# from nltk.metrics.distance  import edit_distance
# from nltk.metrics.distance import jaccard_distance
# from nltk.util import ngrams
# import collections

# k = 0 #retrieve top k+1 words for edit distance
# edited_query_words = []

def boolean_and(left_op, right_op):
    &#34;&#34;&#34;
    Simple boolean bitwise AND operation.
    Args:
    `left_op`: left side for the bitwise operation

    `right_op`: right side for the bitwise operation

    Return:
    
    The result bitwise AND of the left and right as given arguments.

    &#34;&#34;&#34;
    bitwise_op = [w1 &amp; w2 for (w1,w2) in zip(left_op,right_op)]
    return bitwise_op

def boolean_or(left_op, right_op):

    &#34;&#34;&#34;
    Simple boolean bitwise OR operation.
    Args:
    `left_op`: left side for the bitwise operation

    `right_op`: right side for the bitwise operation

    Return:
    
    The result bitwise OR of the left and right as given arguments.

    &#34;&#34;&#34;
    bitwise_op = [w1 | w2 for (w1,w2) in zip(left_op,right_op)]
    return bitwise_op

def boolean_not(right_op):
    &#34;&#34;&#34;
    Simple boolean bitwise NOT operation.

    Args:
    `left_op`: left side for the bitwise operation

    `right_op`: right side for the bitwise operation

    Return:
    
    The result bitwise NOT of the left and right as given arguments.

    &#34;&#34;&#34;
    bitwise_op = [not w1 for w1 in right_op]
    bitwise_op = [int(b == True) for b in bitwise_op]
    return bitwise_op

def shunting_yard(infix_tokens):
    &#34;&#34;&#34;
    Definition for the precedence for the boolean operators such as AND, OR and NOT
    Also defines precedence for &#39;)&#39; and &#39;(&#39; brackets.
    
    Args:
    `infix_tokens`: Infix sequence of the connecting words which are AND, OR and NOT.

    Returns:
    The result after modifying the given tokens.
    &#34;&#34;&#34;
    # define precedences
    precedence = {}
    precedence[&#39;or&#39;] = 1
    precedence[&#39;and&#39;] = 2
    precedence[&#39;not&#39;] = 3
    precedence[&#39;(&#39;] = 0
    precedence[&#39;)&#39;] = 0    

    # declare data strucures
    output = []
    operator_stack = []

    # while there are tokens to be read
    for token in infix_tokens:
        
        # if left bracket
        if (token == &#39;(&#39;):
            operator_stack.append(token)
        
        # if right bracket, pop all operators from operator stack onto output until we hit left bracket
        elif (token == &#39;)&#39;):
            operator = operator_stack.pop()
            while operator != &#39;(&#39;:
                output.append(operator)
                operator = operator_stack.pop()
        
        # if operator, pop operators from operator stack to queue if they are of higher precedence
        elif (token in precedence):
            # if operator stack is not empty
            if (operator_stack):
                current_operator = operator_stack[-1]
                while (operator_stack and precedence[current_operator] &gt; precedence[token]):
                    output.append(operator_stack.pop())
                    if (operator_stack):
                        current_operator = operator_stack[-1]

            operator_stack.append(token) # add token to stack
        
        # else if operands, add to output list
        else:
            output.append(token.lower())

    # while there are still operators on the stack, pop them into the queue
    while (operator_stack):
        output.append(operator_stack.pop())
    # print (&#39;postfix:&#39;, output)  # check
    return output


def process_query(query):
  &#34;&#34;&#34;
    Utility function for processing the user query
  &#34;&#34;&#34;
  query = query.lower()
  query = query.replace(&#39;(&#39;, &#39;( &#39;)
  query = query.replace(&#39;)&#39;, &#39; )&#39;)
  query = query.split(&#39; &#39;)

  print(&#34;query in process_query&#34;, query)

  k = 0 #retrieve top k+1 words for edit distance
  connecting_words = []
  different_words = []
  zeroes_and_ones = []
  edited_query_words = []
  zeroes_and_ones_of_all_words = []
  total_files = len(files_with_index)

  # for word in query:
  #     if word != &#34;and&#34; and word != &#34;or&#34; and word != &#34;not&#34;:
  #         different_words.append(word)
  #     else:
  #         connecting_words.append(word)
  skip_edit_distance = [0]*len(query)
  # print(skip_edit_distance)

  #if in unique words
  for i, query_word in enumerate(query): 
      # print(query_word)
      if query_word != &#34;and&#34; and query_word != &#34;or&#34; and query_word != &#34;not&#34; and query_word != &#39;(&#39; and query_word != &#39;)&#39;:
        if query_word not in unique_words_all:
            if &#34;*&#34; in query_word:
                skip_edit_distance[i] = 0
            else:
                skip_edit_distance[i] = 1
            # print(&#34;unique available :&gt; zeroes_and_ones for &#34;,query_word)
  # print(skip_edit_distance)

  # if not in unique words
  for i,entry in enumerate(query):
      # print(i,entry)
      if skip_edit_distance[i]:
              temp = [(edit_distance(entry, w,substitution_cost = 2,transpositions = True),w) for w in unique_words_all]
              all_words_sorted = sorted(temp)
              # print(&#34;all_words_sorted :&#34;, all_words_sorted[0:10])
              # edited_query_words.append(all_words_sorted[0], i)
              query[i] = all_words_sorted[0][1]

  print(&#34;edited_query_words after edit distance: &#34;,query)
  # for i,entry in enumerate(different_words):
  #   if i in edited_query_words[1]:

  result_stack = []
  print(&#34;query :&#34;,query)
  postfix_queue = collections.deque(shunting_yard(query))
  # print(postfix_queue)
  result = []
  while postfix_queue:
    # print(result_stack)
    # print(postfix_queue)
    # result_stack.append(result)
    token = postfix_queue.popleft()
    result = []
    temp = []
    temp = [0] * total_files
    # print(total_files)
    if (token != &#39;and&#39; and token != &#39;or&#39; and token != &#39;not&#39;):
      if &#34;*&#34; in token:
          result = wildcard_search(token) 
          print(&#34;result of OR of wildcard: &#34;,result)
      else:  
        # token = PorterStemmer().stem(token)
        print(token)
        if token in unique_words_all:
          # print(token)
          linkedlist = linked_list_data[token].head
          while linkedlist.nextval is not None:
            temp[linkedlist.nextval.doc - 1] = 1
            linkedlist = linkedlist.nextval
            # print(temp)
          result = temp
          # print(result)
    elif (token == &#39;and&#39;):
      right_op = result_stack.pop()
      left_op = result_stack.pop()
      result = boolean_and(left_op, right_op)
    elif (token == &#39;or&#39;):
      right_op = result_stack.pop()
      left_op = result_stack.pop()
      result = boolean_or(left_op, right_op)
    elif (token == &#39;not&#39;):
      right_op = result_stack.pop()
      result = boolean_not(right_op)
    else:
        if token!=&#34;)&#34; or token!=&#34;)&#34; or token!=&#39;&#39;:
            token = PorterStemmer().stem(token)
    # print(result)
    result_stack.append(result)
    # print(result_stack)
  # if len(result_stack) != 1 : print(&#34;Error: result_stack. Please check query&#34;)
  return result_stack.pop()


def do_quering(query):
    &#34;&#34;&#34;
    Process query given by the user

  Args:
  `query`: user query
  
  Returns:
  The final stack after processing the query such as lementisation, stemming, and edit-distance
    
    &#34;&#34;&#34;
    # query = input(&#39;Enter your query: &#39;)
    # print(process_query(query))
    res = process_query(query)
    print(&#34;Final result : &#34;,res)
    cnt = 1
    files = []
    for index in res:
        if index == 1:
            files.append(files_with_index[cnt])
        cnt = cnt+1
    if len(files):
        print(files)
    else:
        print(&#34;No such word present in the documents&#34;)
    # process_query(query)

# do_quering(&#34;(julius and caesar)&#34;)

# do_quering(&#34;call or not (julius and caesar)&#34;)

# do_quering(&#34;call or not (julius and caesar) and a*&#34;)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="ir_assignment1.boolean_and"><code class="name flex">
<span>def <span class="ident">boolean_and</span></span>(<span>left_op, right_op)</span>
</code></dt>
<dd>
<div class="desc"><p>Simple boolean bitwise AND operation.
Args:
<code>left_op</code>: left side for the bitwise operation</p>
<p><code>right_op</code>: right side for the bitwise operation</p>
<p>Return:</p>
<p>The result bitwise AND of the left and right as given arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_and(left_op, right_op):
    &#34;&#34;&#34;
    Simple boolean bitwise AND operation.
    Args:
    `left_op`: left side for the bitwise operation

    `right_op`: right side for the bitwise operation

    Return:
    
    The result bitwise AND of the left and right as given arguments.

    &#34;&#34;&#34;
    bitwise_op = [w1 &amp; w2 for (w1,w2) in zip(left_op,right_op)]
    return bitwise_op</code></pre>
</details>
</dd>
<dt id="ir_assignment1.boolean_not"><code class="name flex">
<span>def <span class="ident">boolean_not</span></span>(<span>right_op)</span>
</code></dt>
<dd>
<div class="desc"><p>Simple boolean bitwise NOT operation.</p>
<p>Args:
<code>left_op</code>: left side for the bitwise operation</p>
<p><code>right_op</code>: right side for the bitwise operation</p>
<p>Return:</p>
<p>The result bitwise NOT of the left and right as given arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_not(right_op):
    &#34;&#34;&#34;
    Simple boolean bitwise NOT operation.

    Args:
    `left_op`: left side for the bitwise operation

    `right_op`: right side for the bitwise operation

    Return:
    
    The result bitwise NOT of the left and right as given arguments.

    &#34;&#34;&#34;
    bitwise_op = [not w1 for w1 in right_op]
    bitwise_op = [int(b == True) for b in bitwise_op]
    return bitwise_op</code></pre>
</details>
</dd>
<dt id="ir_assignment1.boolean_or"><code class="name flex">
<span>def <span class="ident">boolean_or</span></span>(<span>left_op, right_op)</span>
</code></dt>
<dd>
<div class="desc"><p>Simple boolean bitwise OR operation.
Args:
<code>left_op</code>: left side for the bitwise operation</p>
<p><code>right_op</code>: right side for the bitwise operation</p>
<p>Return:</p>
<p>The result bitwise OR of the left and right as given arguments.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def boolean_or(left_op, right_op):

    &#34;&#34;&#34;
    Simple boolean bitwise OR operation.
    Args:
    `left_op`: left side for the bitwise operation

    `right_op`: right side for the bitwise operation

    Return:
    
    The result bitwise OR of the left and right as given arguments.

    &#34;&#34;&#34;
    bitwise_op = [w1 | w2 for (w1,w2) in zip(left_op,right_op)]
    return bitwise_op</code></pre>
</details>
</dd>
<dt id="ir_assignment1.do_quering"><code class="name flex">
<span>def <span class="ident">do_quering</span></span>(<span>query)</span>
</code></dt>
<dd>
<div class="desc"><p>Process query given by the user</p>
<p>Args:
<code>query</code>: user query</p>
<p>Returns:
The final stack after processing the query such as lementisation, stemming, and edit-distance</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def do_quering(query):
    &#34;&#34;&#34;
    Process query given by the user

  Args:
  `query`: user query
  
  Returns:
  The final stack after processing the query such as lementisation, stemming, and edit-distance
    
    &#34;&#34;&#34;
    # query = input(&#39;Enter your query: &#39;)
    # print(process_query(query))
    res = process_query(query)
    print(&#34;Final result : &#34;,res)
    cnt = 1
    files = []
    for index in res:
        if index == 1:
            files.append(files_with_index[cnt])
        cnt = cnt+1
    if len(files):
        print(files)
    else:
        print(&#34;No such word present in the documents&#34;)</code></pre>
</details>
</dd>
<dt id="ir_assignment1.fileOpeningProcessing"><code class="name flex">
<span>def <span class="ident">fileOpeningProcessing</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Opens
the file and sends for preprocessing
such as removing special characters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def fileOpeningProcessing():
    &#34;&#34;&#34;
    Opens  the file and sends for preprocessing
    such as removing special characters.

    &#34;&#34;&#34;
    all_words = []
    dict_global = {}
    # file_folder = &#39;/home/fb/Desktop/IR/assignment/IR_CORPUS&#39;
    idx = 1
    files_with_index = {}
    # list_files = [f for f in glob.glob(&#39;/content/IR_CORPUS/*.txt&#39;)]
    list_files = [f for f in glob.glob(&#39;/content/drive/MyDrive/IR_ASSIGNMENT/IR_CORPUS/*.txt&#39;)]

    # for file in glob.glob(file_folder):
    for file in list_files:
        # print(file)
        # print(idx)
        fname = file
        file = open(file , &#34;r&#34;)
        text = file.read()
        text = remove_special_characters(text)
        text = re.sub(re.compile(&#39;\d&#39;),&#39;&#39;,text)
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        words = [word for word in words if len(words)&gt;1]
        words = [word.lower() for word in words]
        words = [word for word in words if word not in Stopwords]
        # words = [PorterStemmer().stem(word) for word in words]
        dict_global.update(finding_all_unique_words_and_freq(words))
        files_with_index[idx] = os.path.basename(fname)
        idx = idx + 1
        
    unique_words_all = set(dict_global.keys())</code></pre>
</details>
</dd>
<dt id="ir_assignment1.finding_all_unique_words_and_freq"><code class="name flex">
<span>def <span class="ident">finding_all_unique_words_and_freq</span></span>(<span>words)</span>
</code></dt>
<dd>
<div class="desc"><p>finds all the unique words and their frequency from the given dataset of IR Corpus</p>
<p>Args:</p>
<p><code>words</code>:
words processed from the document</p>
<p>Returns:</p>
<p>The frequency of the given word in the function</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finding_all_unique_words_and_freq(words):
    &#34;&#34;&#34;
    finds all the unique words and their frequency from the given dataset of IR Corpus
    
    Args:
    
    `words`:  words processed from the document

    Returns:
    
    The frequency of the given word in the function
    &#34;&#34;&#34;
    words_unique = []
    word_freq = {}
    for word in words:
        if word not in words_unique:
            words_unique.append(word)
    for word in words_unique:
        word_freq[word] = words.count(word)
    return word_freq</code></pre>
</details>
</dd>
<dt id="ir_assignment1.finding_freq_of_word_in_doc"><code class="name flex">
<span>def <span class="ident">finding_freq_of_word_in_doc</span></span>(<span>word, words)</span>
</code></dt>
<dd>
<div class="desc"><p>Counts the frequency fof the word in the document</p>
<p>Args:
<code>word</code>: word form the document</p>
<p><code>words</code>:counts the frequency for the given in the document</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def finding_freq_of_word_in_doc(word,words):
    &#34;&#34;&#34;
    Counts the frequency fof the word in the document

    Args:
    `word`: word form the document

    `words`:counts the frequency for the given in the document
    &#34;&#34;&#34;
    freq = words.count(word)</code></pre>
</details>
</dd>
<dt id="ir_assignment1.preprocessing"><code class="name flex">
<span>def <span class="ident">preprocessing</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>The following function preprocesses the document IR Corpus.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def preprocessing():
    &#34;&#34;&#34;
    The following function preprocesses the document IR Corpus.
    &#34;&#34;&#34;
    linked_list_data = {}
    for word in unique_words_all:
        linked_list_data[word] = SlinkedList()
        linked_list_data[word].head = Node(1,Node)
    word_freq_in_doc = {}
    idx = 1
    # list_files = [f for f in glob.glob(&#39;/content/IR_CORPUS/*txt&#39;)]
    list_files = [f for f in glob.glob(&#39;/content/drive/MyDrive/IR_ASSIGNMENT/IR_CORPUS/*.txt&#39;)]

    # for file in glob.glob(file_folder):
    for file in list_files:
        file = open(file, &#34;r&#34;)
        text = file.read()
        # print(text)
        # print(idx)
        text = remove_special_characters(text)
        text = re.sub(re.compile(&#39;\d&#39;),&#39;&#39;,text)
        sentences = sent_tokenize(text)
        words = word_tokenize(text)
        words = [word for word in words if len(words)&gt;1]
        words = [word.lower() for word in words]
        words = [word for word in words if word not in Stopwords]
        # words = [PorterStemmer().stem(word) for word in words]
        word_freq_in_doc = finding_all_unique_words_and_freq(words)
        for word in word_freq_in_doc.keys():
            linked_list = linked_list_data[word].head
            while linked_list.nextval is not None:
                linked_list = linked_list.nextval
            linked_list.nextval = Node(idx ,word_freq_in_doc[word])
        idx = idx + 1</code></pre>
</details>
</dd>
<dt id="ir_assignment1.process_query"><code class="name flex">
<span>def <span class="ident">process_query</span></span>(<span>query)</span>
</code></dt>
<dd>
<div class="desc"><p>Utility function for processing the user query</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def process_query(query):
  &#34;&#34;&#34;
    Utility function for processing the user query
  &#34;&#34;&#34;
  query = query.lower()
  query = query.replace(&#39;(&#39;, &#39;( &#39;)
  query = query.replace(&#39;)&#39;, &#39; )&#39;)
  query = query.split(&#39; &#39;)

  print(&#34;query in process_query&#34;, query)

  k = 0 #retrieve top k+1 words for edit distance
  connecting_words = []
  different_words = []
  zeroes_and_ones = []
  edited_query_words = []
  zeroes_and_ones_of_all_words = []
  total_files = len(files_with_index)

  # for word in query:
  #     if word != &#34;and&#34; and word != &#34;or&#34; and word != &#34;not&#34;:
  #         different_words.append(word)
  #     else:
  #         connecting_words.append(word)
  skip_edit_distance = [0]*len(query)
  # print(skip_edit_distance)

  #if in unique words
  for i, query_word in enumerate(query): 
      # print(query_word)
      if query_word != &#34;and&#34; and query_word != &#34;or&#34; and query_word != &#34;not&#34; and query_word != &#39;(&#39; and query_word != &#39;)&#39;:
        if query_word not in unique_words_all:
            if &#34;*&#34; in query_word:
                skip_edit_distance[i] = 0
            else:
                skip_edit_distance[i] = 1
            # print(&#34;unique available :&gt; zeroes_and_ones for &#34;,query_word)
  # print(skip_edit_distance)

  # if not in unique words
  for i,entry in enumerate(query):
      # print(i,entry)
      if skip_edit_distance[i]:
              temp = [(edit_distance(entry, w,substitution_cost = 2,transpositions = True),w) for w in unique_words_all]
              all_words_sorted = sorted(temp)
              # print(&#34;all_words_sorted :&#34;, all_words_sorted[0:10])
              # edited_query_words.append(all_words_sorted[0], i)
              query[i] = all_words_sorted[0][1]

  print(&#34;edited_query_words after edit distance: &#34;,query)
  # for i,entry in enumerate(different_words):
  #   if i in edited_query_words[1]:

  result_stack = []
  print(&#34;query :&#34;,query)
  postfix_queue = collections.deque(shunting_yard(query))
  # print(postfix_queue)
  result = []
  while postfix_queue:
    # print(result_stack)
    # print(postfix_queue)
    # result_stack.append(result)
    token = postfix_queue.popleft()
    result = []
    temp = []
    temp = [0] * total_files
    # print(total_files)
    if (token != &#39;and&#39; and token != &#39;or&#39; and token != &#39;not&#39;):
      if &#34;*&#34; in token:
          result = wildcard_search(token) 
          print(&#34;result of OR of wildcard: &#34;,result)
      else:  
        # token = PorterStemmer().stem(token)
        print(token)
        if token in unique_words_all:
          # print(token)
          linkedlist = linked_list_data[token].head
          while linkedlist.nextval is not None:
            temp[linkedlist.nextval.doc - 1] = 1
            linkedlist = linkedlist.nextval
            # print(temp)
          result = temp
          # print(result)
    elif (token == &#39;and&#39;):
      right_op = result_stack.pop()
      left_op = result_stack.pop()
      result = boolean_and(left_op, right_op)
    elif (token == &#39;or&#39;):
      right_op = result_stack.pop()
      left_op = result_stack.pop()
      result = boolean_or(left_op, right_op)
    elif (token == &#39;not&#39;):
      right_op = result_stack.pop()
      result = boolean_not(right_op)
    else:
        if token!=&#34;)&#34; or token!=&#34;)&#34; or token!=&#39;&#39;:
            token = PorterStemmer().stem(token)
    # print(result)
    result_stack.append(result)
    # print(result_stack)
  # if len(result_stack) != 1 : print(&#34;Error: result_stack. Please check query&#34;)
  return result_stack.pop()</code></pre>
</details>
</dd>
<dt id="ir_assignment1.remove_special_characters"><code class="name flex">
<span>def <span class="ident">remove_special_characters</span></span>(<span>text)</span>
</code></dt>
<dd>
<div class="desc"><p>This function removes the special charactes if any in the document so that it is easy to process the document.</p>
<p>Return:</p>
<p>The text after removing the special characters.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def remove_special_characters(text):
    &#34;&#34;&#34;
    This function removes the special charactes if any in the document so that it is easy to process the document.

    Return:

    The text after removing the special characters. 
    &#34;&#34;&#34;
    regex = re.compile(&#39;[^a-zA-Z0-9\s]&#39;)
    text_returned = re.sub(regex,&#39;&#39;,text)
    return text_returned</code></pre>
</details>
</dd>
<dt id="ir_assignment1.shunting_yard"><code class="name flex">
<span>def <span class="ident">shunting_yard</span></span>(<span>infix_tokens)</span>
</code></dt>
<dd>
<div class="desc"><p>Definition for the precedence for the boolean operators such as AND, OR and NOT
Also defines precedence for ')' and '(' brackets.</p>
<p>Args:
<code>infix_tokens</code>: Infix sequence of the connecting words which are AND, OR and NOT.</p>
<p>Returns:
The result after modifying the given tokens.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def shunting_yard(infix_tokens):
    &#34;&#34;&#34;
    Definition for the precedence for the boolean operators such as AND, OR and NOT
    Also defines precedence for &#39;)&#39; and &#39;(&#39; brackets.
    
    Args:
    `infix_tokens`: Infix sequence of the connecting words which are AND, OR and NOT.

    Returns:
    The result after modifying the given tokens.
    &#34;&#34;&#34;
    # define precedences
    precedence = {}
    precedence[&#39;or&#39;] = 1
    precedence[&#39;and&#39;] = 2
    precedence[&#39;not&#39;] = 3
    precedence[&#39;(&#39;] = 0
    precedence[&#39;)&#39;] = 0    

    # declare data strucures
    output = []
    operator_stack = []

    # while there are tokens to be read
    for token in infix_tokens:
        
        # if left bracket
        if (token == &#39;(&#39;):
            operator_stack.append(token)
        
        # if right bracket, pop all operators from operator stack onto output until we hit left bracket
        elif (token == &#39;)&#39;):
            operator = operator_stack.pop()
            while operator != &#39;(&#39;:
                output.append(operator)
                operator = operator_stack.pop()
        
        # if operator, pop operators from operator stack to queue if they are of higher precedence
        elif (token in precedence):
            # if operator stack is not empty
            if (operator_stack):
                current_operator = operator_stack[-1]
                while (operator_stack and precedence[current_operator] &gt; precedence[token]):
                    output.append(operator_stack.pop())
                    if (operator_stack):
                        current_operator = operator_stack[-1]

            operator_stack.append(token) # add token to stack
        
        # else if operands, add to output list
        else:
            output.append(token.lower())

    # while there are still operators on the stack, pop them into the queue
    while (operator_stack):
        output.append(operator_stack.pop())
    # print (&#39;postfix:&#39;, output)  # check
    return output</code></pre>
</details>
</dd>
<dt id="ir_assignment1.wildcard"><code class="name flex">
<span>def <span class="ident">wildcard</span></span>(<span>word)</span>
</code></dt>
<dd>
<div class="desc"><p>Utility function for wildcard searching</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wildcard(word):
    &#34;&#34;&#34;
    Utility function for wildcard searching
    &#34;&#34;&#34;
    wildcard_words = []
    word_list = word.split(&#34;*&#34;)
    re_query = &#34;^&#34;+ word_list[0]+&#34;.*&#34;+word_list[1]+&#34;$&#34;
    # print(re_query)
    for word in unique_words_all:
        if re.search(re_query,word):
            wildcard_words.append(word)        
    return wildcard_words</code></pre>
</details>
</dd>
<dt id="ir_assignment1.wildcard_search"><code class="name flex">
<span>def <span class="ident">wildcard_search</span></span>(<span>word)</span>
</code></dt>
<dd>
<div class="desc"><p>This function searches the query using wildcard searching if the token/word contains * in it.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def wildcard_search(word):
    &#34;&#34;&#34;
    This function searches the query using wildcard searching if the token/word contains * in it. 
    &#34;&#34;&#34;
    total_files = len(files_with_index)
    zeroes_and_ones_of_all_words = []
    # word = &#34;br*us&#34;
    #if in unique words
    i = 0
    for query_word in wildcard(word): 
        # print(query_word)
        if query_word in unique_words_all:
            # skip_edit_distance[i] = 1
            zeroes_and_ones = [0] * total_files
            linkedlist = linked_list_data[query_word].head
            # print(&#34;word:&#34;,word)
            while linkedlist.nextval is not None:
                zeroes_and_ones[linkedlist.nextval.doc - 1] = 1
                linkedlist = linkedlist.nextval
            print(&#34;unique available :&gt; zeroes_and_ones for &#34;,query_word,&#34; :&gt;&#34;,zeroes_and_ones)
            zeroes_and_ones_of_all_words.append(zeroes_and_ones)
        i+=1

    res = [0] * total_files
    for zr_and_ons in zeroes_and_ones_of_all_words:
        res = boolean_or(res,zr_and_ons)
    return res</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="ir_assignment1.Node"><code class="flex name class">
<span>class <span class="ident">Node</span></span>
<span>(</span><span>docId, freq=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Class for Node</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Node:
    &#34;&#34;&#34;
    Class for Node
    &#34;&#34;&#34;
    def __init__(self ,docId, freq = None):
        self.freq = freq
        self.doc = docId
        self.nextval = None</code></pre>
</details>
</dd>
<dt id="ir_assignment1.SlinkedList"><code class="flex name class">
<span>class <span class="ident">SlinkedList</span></span>
<span>(</span><span>head=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SlinkedList:
    def __init__(self ,head = None):
        self.head = head</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="ir_assignment1.boolean_and" href="#ir_assignment1.boolean_and">boolean_and</a></code></li>
<li><code><a title="ir_assignment1.boolean_not" href="#ir_assignment1.boolean_not">boolean_not</a></code></li>
<li><code><a title="ir_assignment1.boolean_or" href="#ir_assignment1.boolean_or">boolean_or</a></code></li>
<li><code><a title="ir_assignment1.do_quering" href="#ir_assignment1.do_quering">do_quering</a></code></li>
<li><code><a title="ir_assignment1.fileOpeningProcessing" href="#ir_assignment1.fileOpeningProcessing">fileOpeningProcessing</a></code></li>
<li><code><a title="ir_assignment1.finding_all_unique_words_and_freq" href="#ir_assignment1.finding_all_unique_words_and_freq">finding_all_unique_words_and_freq</a></code></li>
<li><code><a title="ir_assignment1.finding_freq_of_word_in_doc" href="#ir_assignment1.finding_freq_of_word_in_doc">finding_freq_of_word_in_doc</a></code></li>
<li><code><a title="ir_assignment1.preprocessing" href="#ir_assignment1.preprocessing">preprocessing</a></code></li>
<li><code><a title="ir_assignment1.process_query" href="#ir_assignment1.process_query">process_query</a></code></li>
<li><code><a title="ir_assignment1.remove_special_characters" href="#ir_assignment1.remove_special_characters">remove_special_characters</a></code></li>
<li><code><a title="ir_assignment1.shunting_yard" href="#ir_assignment1.shunting_yard">shunting_yard</a></code></li>
<li><code><a title="ir_assignment1.wildcard" href="#ir_assignment1.wildcard">wildcard</a></code></li>
<li><code><a title="ir_assignment1.wildcard_search" href="#ir_assignment1.wildcard_search">wildcard_search</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="ir_assignment1.Node" href="#ir_assignment1.Node">Node</a></code></h4>
</li>
<li>
<h4><code><a title="ir_assignment1.SlinkedList" href="#ir_assignment1.SlinkedList">SlinkedList</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>